{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note \n",
    "This notebook requires unpacking the `data/json.zip` file included in this repo. This file is about 18 GB when completely unzipped. If you would rather not unzip `data/json.zip` to your own machine, this repo already includes all doc-terms and metadata files you need to replicate our analysis. It is not necessary to run this notebook unless you want to reproduce these derived files. See the repo `README.md` for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook unpacks json data included in this repo and prepares this data for analysis using the other notebooks in this code repo. All methods that require access to fully unpacked json documents are included in this notebook. See this module's `README.md` for more information on usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n",
    "\n",
    "Thie code block below imports required Python modules and the `prepare-data.py` script. It is also where you should define the filepaths for the various files you will need to use or create.\n",
    "\n",
    "Variables not defined below:\n",
    "\n",
    "* `log_file` = the path to the log file the script will create if there are errors. The script only creates a log file if it can't process a json document. Again, you should include the name of the log file itself in its path. By default, this log file appears in this module as `prepare-data-log.txt`.\n",
    "* `stoplist_file` = the path to the stoplist file you will be using. This is set to the stoplist included in this code repo by default.\n",
    "* `strip_digits` = a Boolean value that tells the code whether to strip digits. To replicate our process, you should set this to `False`. If you set it to `True`, you will end up with slightly different input data than we used.\n",
    "* `metadata_csv_file`, `metadata_file_reorder`, `browser_meta_file_temp` and `browser_meta_file` = The metadata files the code below will create for use in dfr-browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Import scripts\n",
    "%run scripts/prepare-data.py\n",
    "\n",
    "# Define paths \n",
    "# directory this notebook is in\n",
    "current_dir       = %pwd\n",
    "# directory of this repo on your machine\n",
    "module_dir        = str(Path(current_dir).parent)\n",
    "# directory of repo data\n",
    "data_dir          = module_dir + '/data'\n",
    "# zip of all json files\n",
    "json_zip          = data_dir + '/json.zip'\n",
    "# directory of json files this notebook will create\n",
    "json_dir          = data_dir + '/json'\n",
    "# all other variables are defined above\n",
    "log_file          = current_dir + '/prepare-data-log.txt'\n",
    "stoplist_file     = 'stoplist.txt'\n",
    "strip_digits      = False\n",
    "metadata_csv_file      = metadata_dir + '/metadata-dfrb.csv'\n",
    "metadata_file_reorder  = metadata_dir + '/metadata-dfrb.csv'\n",
    "browser_meta_file_temp = metadata_dir + '/meta.temp.csv'\n",
    "browser_meta_file      = metadata_dir + '/meta.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Unzip Json Directory\n",
    "\n",
    "This cell unpacks the `json.zip` archive into a directory in this repo's `data` diretory: `data/json`. `json.zip` is 1.01 GB; completely unzipped it's about 18 GB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip archive\n",
    "extract_data(json_zip, data_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Doc-Terms Files Needed for Analysis\n",
    "\n",
    "The code in this section reads data from specified json files, takes only the bag of words data from each json file, and copies it to a single doc-terms file to be used as input for the other analysis methods included in this code repo. If you have already created a file named the exact same thing as you name the `import_file_path` file below, this code will erase that already existing file and create a new one. The same goes for the `log_file` file.\n",
    "\n",
    "This can take awhile, depending on the size of the collection you are preparing for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, select the collection you want to prepare data for. See the repo `README.md` for information about collections. Setting this value will determine the path of the following 2 files:\n",
    "\n",
    "* `import_file_path` = the path to the doc-terms file you want to produce via this notebook. This path is set to include the name of the collection you select by default. We have provided doc-terms files in the `data/doc-terms` directory in this repo. Therefore, the code below will create a new doc-terms file for the collection you select (the filename will include the word `'new'` at the end).\n",
    "* `filelist_file` = the path to the list of files naming the documents you want to prepare for analysis. This will default to the list for the collection you select. You should use this option if you only want to prepare some files in the `json` directory for analysis (but not all). If using this option, your list of filenames should inlude one filename per line, and your list must be in plain-text form. The `data/filelists` directory contains lists of files that correspond to the ways we have divided up our corpus for analysis. If you wish to prepare all documents in the json directory for analysis (as one big file), you can select `filelist_file = None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# options are 'c14', 'c18', 'c20', 'c21', 'c33', 'c36', 'c36a', 'c37'\n",
    "collection = 'c33'\n",
    "\n",
    "import_file_path = data_dir + '/doc-terms/' + collection + '-doc-terms-new.txt'\n",
    "filelist_file = data_dir + '/filelists/' + collection + '-filenames.txt'\n",
    "\n",
    "## uncomment if you don't want to use a filelist\n",
    "# filelist_file = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Produce doc-terms files for specified collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "prepare_data(json_dir, import_file_path, strip_digits, stoplist_file, log_file, filelist_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Metadata: Create dfr-browser csv metadata files from json documents (required for dfr-browser)\n",
    "\n",
    "Run the below cell if you wish to replicate the process we used to create dfr-browser metadata files. If a folder with the same name as the value you have set for `metadata_dir` above already exists, this code will delete and recreate it.\n",
    "\n",
    "This cell opens up each json in the json directory on the filelist you designated above (`filelist_file`) and grabs the metadata information dfr-browser needs. It creates the `metadata_csv_file`, `browser_meta_file_temp`, and `browser_meta_file_temp` files defined above. These files will be saved in the location you designated above for `metadata_dir`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, select the collection you want to prepare a metadata file for. See the repo `README.md` for information about collections. Setting this value will determine the path of the following 2 files:\n",
    "\n",
    "* `filelist_file` = the path to the list of files naming the documents you want to prepare for analysis. This will default to the list for the collection you select. The `data/filelists` directory contains lists of files that correspond to the ways we have divided up our corpus for analysis. You must select a filelist to use this code.\n",
    "* `metadata_dir` = The location where the dfr-browser metadata files you create using this notebook will be stored. By default, this is set to include the name of the collection you select. If you create metadata files for collections 33 or 36 using this notebook, they will replace the `data/metadata-c33` and/or `data/metadata-c36` included with this repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# options are 'c14', 'c18', 'c20', 'c21', 'c33', 'c36', 'c36a', 'c37'\n",
    "collection = 'c33'\n",
    "\n",
    "filelist_file = data_dir + '/filelists/' + collection + '-filenames.txt'\n",
    "metadata_dir = data_dir + '/metadata-' + collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Produce metadata files for specified collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Running this code will delete old metadata files and create new metadata folder within project_data directory\n",
    "\n",
    "dfrb_metadata(metadata_dir, metadata_csv_file, browser_meta_file_temp, browser_meta_file, json_dir, filelist_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
